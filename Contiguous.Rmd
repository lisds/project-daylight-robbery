---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=6)
import scipy.stats as sps

from projtools.data import Data

data = Data()
```

# Seasonal Changes Regression Analysis


As can be seen in the previous page, there are some statistically unlikey changes in some crimes based on the time of year. In this section of the notebook we will look into the factors that change with the transition from Greenwich Mean Time (GMT) to British Summer Time (BST).


## Weather - Linear Regression
One of the things that changes alongside the time zone, is the weather. It could be that the switch from GMT to BST and vice versa leads to changes in the crimes committed due to correlation with the changes in weather. One of the offences most affected by the change in time zone was bike thefts, with many more recorded offenc. So, we will use that as a case study.


### Rainfall
It is not unreasonable to assume that the weather influences the prevalence of some crimes, such as theft. It stands to reason that if it is wetter, people will not want to cycle as much. This means there they may be less bikes around for people to steal.

```{python}
rain_dict = {'Months':['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], 'Avg Rainfall (mm)':[58.83, 44.96, 38.78, 42.31, 45.91, 47.25, 45.80, 52.78, 49.61, 65.07, 66.63, 57.05]}
rain_df = pd.DataFrame(data=rain_dict)             
rain_df
```

The dataframe above shows the average rainfall in London per month using data collected from 1991-2020 by the [Met Office](https://metoffice.gov.uk/research/climate/maps-and-data/uk-climate-averages/gcpsvg3nc).

```{python}
rain_df.plot()
plt.ylabel('mm')
plt.xlabel('Month')
```

This data is more easily visualised with the graph above. It seems that rainfall peaks around September and October, with the lowest value in February. According to our hypothesis, this means that some crimes will follow an inverse of this trend.

```{python}
theft = data.borough.loc[:, 'Bicycle Theft', :].T.groupby(level='month').sum().T.sum()
theft_df = theft.to_frame('Bicycle Theft')
theft_df.plot()
plt.ylabel('Recorded Bicycle Thefts')
theft_df
```

As you can see from the graph above, bicycle thefts peak in July and remain high until October. This does not fit with the hypothesis so far. Now we will do a linear regression to see if there is a correlation between the rainfall and bike thefts or not.

```{python}
rainfall = np.array(rain_df['Avg Rainfall (mm)'])
bike_thefts = np.array(theft_df['Bicycle Theft'])
bike_thefts
```

In this analysis, bike theft will be the outcome variable, while rainfall will be the predictor variable.

`bike_thefts` $ = b $ `rainfall` $+$ $  \text{c} + \vec{\varepsilon} $

To show an strong predictive relationship, we would like the slope (b) to be relatively high and the error (e) vector to be low.

```{python}
rainfall
bike_thefts
res = sps.linregress(rainfall, bike_thefts)
b = res.slope
c = res.intercept
print('slope:', b)
print('intercept:', c)
```

The slope has a relatively low value, suggesting that if there is any relationship, it is not particularly strong. It suggests that for every mm of rainfall, the bike thefts decrease by 33.

```{python}
fitted = b * rainfall + c
fitted
```

These are the fitted values, which show the values if there was a relationship with the slope and intercept value that was calculated.

```{python}
errors = bike_thefts - fitted
print(errors)
np.sum(errors ** 2)
```

The error values show the difference between the actual values and the fitted values. These error values and sum of squared errors are very high, which suggests that relationship between the two values is not clear/strong.

```{python}
assert np.allclose(bike_thefts, b * rainfall + c + errors)
```

The above code checks that the values for the actual y values are similar to the fitted y values, plus the error values. They should be identical if the calculations have been done properly.

```{python}
def scatter_error(x, y, fitted):
    """
    Function that takes x, y and fitted values and returns a scatter plot with errors.
    Parameters
    ---------------
    x: array of predictor values
    y: array of outcome values
    fitted: array of fitted values
    with errors: shows the error values
    -----------------------------------
    Returns:
    Scatter graph with error values
    """
    plt.scatter(x, y, label='Actual values ($y$)')
   
    plt.plot(x, fitted, 'ro', label='Fitted values ($\hat{y}$)')
    
    n = len(x)
    for i in range(n):
        plt.plot([x[i], x[i]], [fitted[i], y[i]], 'k:')
        
    plt.plot([], [], 'k:', label='Errors ($ \\varepsilon $)')
    
    plt.ylabel('Bike Thefts')
    plt.xlabel('Predictor Variable')
    
    for i, txt in enumerate(rain_df.Months):
        plt.annotate(txt, (x[i], fitted[i]))
    
    plt.legend();
    plt.show()
```

```{python}
scatter_error(rainfall, bike_thefts, fitted)
```

As can be seen by the plot above, there is not a strong predictive relationship between the rainfall in a month and the bike thefts committed. This may be because London gets a lot of rain, so the rainfall per month is relatively consistent.


### Sunshine
Another factor that changes with the season and may influence people's behaviour is sunshine hours.

```{python}
sun_dict = {'Months':['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], 'Sunshine (hours)':[61.09, 78.81, 124.45, 176.67, 207.49, 208.39, 217.81, 202.12, 157.11, 115.17, 70.74, 54.96]}
sun_df = pd.DataFrame(data=sun_dict)             
sun_df
```

The dataframe above shows the average sunshine hours in London per month using data collected from 1991-2020 by the [Met Office](https://metoffice.gov.uk/research/climate/maps-and-data/uk-climate-averages/gcpsvg3nc).

```{python}
sun_df.plot()
plt.ylabel('Hours')
plt.xlabel('Month')
```

```{python}
theft_df.plot()
plt.ylabel('Bike Thefts')
```

The two graphs above show the changes in sunshine hours and bike thefts across the year, respectively.

It seems that the shapes are very similar, which might mean there is a strong predictive relationship between the two.

```{python}
sunshine = np.array(sun_df['Sunshine (hours)'])
```

In this analysis, bike theft will be the outcome variable, while sunshine will be the predictor variable.

`bike_thefts` $ = b $ `sunshine` $+$ $  \text{c} + \vec{\varepsilon} $

As before, to show an strong predictive relationship, we would like the slope (b) to be relatively high and the error (e) vector to be low.

```{python}
res_sun = sps.linregress(sunshine, bike_thefts)
b_sun = res_sun.slope
c_sun = res_sun.intercept
print('slope:', b_sun)
print('intercept:', c_sun)
```

The slope value is higher than before, which means, assuming the error values are lower, that there will be a stronger predictive relationship with sunshine and bike thefts, than rainfall and bike thefts. For every hour more of sunshine, there are 84 more bike thefts.

```{python}
fitted_sun = b_sun * sunshine + c_sun
errors_sun = bike_thefts - fitted_sun
print(errors_sun)
np.sum(errors_sun ** 2)
```

The errors and sum of square errors seem a lot lower than with rainfall, but they are still quite high. This means the relationship could be slightly clearer/stronger.

```{python}
assert np.allclose(bike_thefts, b_sun * sunshine + c_sun + errors_sun)
```

This test has passed, suggesting that the values have been calculated properly.

```{python}
scatter_error(sunshine, bike_thefts, fitted_sun)
```

It seems that there is a stronger predictive relationship present between sunshine hours and bike thefts, than rainfall and bike thefts. Based on the error intervals, it seems that this predictive relationship holds strongest at the extremes; when it is sunniest and darkest.

The relationship is a bit weaker towards the middle values, suggesting that there may be a confounding variable that is responsible for these results.

The clustering follows the BST and GMT groupings, as expected with sunshine hours, with the months in which the change occurs (Oct and Mar) being close together. What is interesting as that despite the fact that Mar and Oct have a very similar number of daylight hours, there is a very big difference in the actual values. This could be the effect of the time zone change.

The end of BST is at the end of Oct, which means for the most of month society has an extra hour of daylight, even though there are less sunshine hours than Mar. The end of GMT is at the end of Mar, which means that there is one less hour of daylight, even though there are more sunshine hours than Oct. It could be that the stark difference between the actual values for the two months are a result of the change in time zones.


### Temperature
Temperature is also a factor that changes as the time zones do, while being a major influence on people's behaviour. People might be more inclined to cycle in BST, as it is warmer.

```{python}
max_temps = np.array([8.42, 8.98, 11.73, 15, 18.37, 21.57, 23.89, 23.40, 20.22, 15.81, 11.47, 8.79])
min_temps = np.array([2.68, 2.65, 4.14, 6.03, 9.08, 12.03, 14.18, 14.06, 11.62, 8.78, 5.26, 3.08])
avg_temps = (max_temps + min_temps) / 2
temp_dict = {'Months':['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], 'Avg Temperature (°C)': avg_temps}
temp_df = pd.DataFrame(data=temp_dict)             
temp_df
```

The dataframe above shows the average temperature in London per month using data collected from 1991-2020 by the [Met Office](https://metoffice.gov.uk/research/climate/maps-and-data/uk-climate-averages/gcpsvg3nc). The data provided by the Met Office was the minimum and maximum temperatures per month. For the purposes of this analysis, I calculated the mean temperature per month using the two values provided.

```{python}
temp_df.plot()
plt.ylabel('Temperature')
plt.xlabel('Month')
```

```{python}
theft_df.plot()
plt.ylabel('Bike Thefts')
```

Like with the sunshine hours, there seems to be a high degree of similarity between the two plots of bike thefts per month and average temparature per month. This could be suggestive of a strong predictive relationship between the two variables.

```{python}
temperature = np.array(temp_df['Avg Temperature (°C)'])
```

In this analysis, bike theft will be the outcome variable, while temperature will be the predictor variable.

`bike_thefts` $ = b $ `temperature` $+$ $  \text{c} + \vec{\varepsilon} $

To show an strong predictive relationship, we would like the slope (b) to be relatively high and the error (e) vector to be low.

```{python}
res_temp = sps.linregress(temperature, bike_thefts)
b_temp = res_temp.slope
c_temp = res_temp.intercept
print('slope:', b_temp)
print('intercept:', c_temp)
```

The value for the slope is ridiculously high. It suggests that for every 1 degree increase in temperature, there are 1207 more bike thefts.

```{python}
fitted_temp = b_temp * temperature + c_temp

errors_temp = bike_thefts - fitted_temp
print(errors_temp)
np.sum(errors_temp ** 2)
```

The error values and sum of squared errors are considerably lower than with rainfall and sunshine, suggesting that the relationship is more accurate predictively.

```{python}
assert np.allclose(bike_thefts, b_temp * temperature + c_temp + errors_temp) #calculations have been done correctly
```

```{python}
scatter_error(temperature, bike_thefts, fitted_temp)
```

There is a very strong predictive relationship between bike thefts and temperature. Similar to the sunshine regression, the GMT and BST months are clustered close together, especially Jan, Feb & Dec and Jul & Aug.

It seems that there are some months for which the error bars are particularly high (Dec, Nov, Oct, Aug). Again, this could be due a confounding variable, such as rainfall or sunshine hours.



